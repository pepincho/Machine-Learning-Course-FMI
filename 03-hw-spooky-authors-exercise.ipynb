{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spooky Author Identification exercise\n",
    "\n",
    "Spooky authors dataset. Mоже да се изтегли от тук:\n",
    "https://www.kaggle.com/c/spooky-author-identification\n",
    "\n",
    "#### Изводи\n",
    "\n",
    "След направените анализи в/у feature-те по време на лекцията на курса можем да стигнем до следните наблюдения и изводи:\n",
    "- ...\n",
    "- ...\n",
    "\n",
    "Също така можем да опитаме някакъв feature engineering, \"почиставне\" и обработка на данните:\n",
    "- ...\n",
    "- ...\n",
    "\n",
    "### Стратегия\n",
    "- ...\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Започваме\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.6/site-packages\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.6/site-packages\n",
      "Requirement already satisfied: ipython in /opt/conda/lib/python3.6/site-packages\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.6/site-packages\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.6/site-packages\n",
      "Requirement already satisfied: pillow in /opt/conda/lib/python3.6/site-packages\n",
      "Collecting mglearn\n",
      "  Downloading mglearn-0.1.6.tar.gz (541kB)\n",
      "\u001b[K    100% |████████████████████████████████| 542kB 944kB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.10 in /opt/conda/lib/python3.6/site-packages (from matplotlib)\n",
      "Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.6/site-packages (from matplotlib)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.6/site-packages (from matplotlib)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.6/site-packages/cycler-0.10.0-py3.6.egg (from matplotlib)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=1.5.6 in /opt/conda/lib/python3.6/site-packages (from matplotlib)\n",
      "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /opt/conda/lib/python3.6/site-packages (from ipython)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.6/site-packages (from ipython)\n",
      "Requirement already satisfied: simplegeneric>0.8 in /opt/conda/lib/python3.6/site-packages (from ipython)\n",
      "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /opt/conda/lib/python3.6/site-packages (from ipython)\n",
      "Requirement already satisfied: setuptools>=18.5 in /opt/conda/lib/python3.6/site-packages (from ipython)\n",
      "Requirement already satisfied: traitlets>=4.2 in /opt/conda/lib/python3.6/site-packages (from ipython)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.6/site-packages (from ipython)\n",
      "Requirement already satisfied: pygments in /opt/conda/lib/python3.6/site-packages (from ipython)\n",
      "Requirement already satisfied: jedi>=0.10 in /opt/conda/lib/python3.6/site-packages (from ipython)\n",
      "Requirement already satisfied: olefile in /opt/conda/lib/python3.6/site-packages (from pillow)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.6/site-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython)\n",
      "Requirement already satisfied: ipython-genutils in /opt/conda/lib/python3.6/site-packages (from traitlets>=4.2->ipython)\n",
      "Building wheels for collected packages: mglearn\n",
      "  Running setup.py bdist_wheel for mglearn ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/jovyan/.cache/pip/wheels/79/8b/2b/17dcfb9c9b044b216a58daea9787a0637cb1ffc5b4c2a78e50\n",
      "Successfully built mglearn\n",
      "Installing collected packages: mglearn\n",
      "Successfully installed mglearn-0.1.6\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy scipy matplotlib ipython scikit-learn pandas pillow mglearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import mglearn\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19579, 2) (8392, 1) (8392, 3)\n",
      "{'author'}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train = pd.read_csv(\"data/spooky-authors/train.zip\", index_col=['id'])\n",
    "test = pd.read_csv(\"data/spooky-authors/test.zip\", index_col=['id'])\n",
    "sample_submission = pd.read_csv(\"data/spooky-authors/sample_submission.zip\", index_col=['id'])\n",
    "\n",
    "print(train.shape, test.shape, sample_submission.shape)\n",
    "print(set(train.columns) - set(test.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>id26305</th>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id17569</th>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id11008</th>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id27763</th>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>MWS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id12958</th>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      text author\n",
       "id                                                               \n",
       "id26305  This process, however, afforded me no means of...    EAP\n",
       "id17569  It never once occurred to me that the fumbling...    HPL\n",
       "id11008  In his left hand was a gold snuff box, from wh...    EAP\n",
       "id27763  How lovely is spring As we looked from Windsor...    MWS\n",
       "id12958  Finding nothing else, not even gold, the Super...    HPL"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.2.5.tar.gz (1.2MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.2MB 558kB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from nltk)\n",
      "Building wheels for collected packages: nltk\n",
      "  Running setup.py bdist_wheel for nltk ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/jovyan/.cache/pip/wheels/18/9c/1f/276bc3f421614062468cb1c9d695e6086d0c73d67ea363c501\n",
      "Successfully built nltk\n",
      "Installing collected packages: nltk\n",
      "Successfully installed nltk-3.2.5\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"features__ngram_range\": [(1,1), (1,2), (1,3)],\n",
    "                      \"features__analyzer\": ['word'],\n",
    "                      \"features__max_df\": [1.0, 0.9, 0.8, 0.7, 0.6, 0.5],\n",
    "                      \"features__min_df\": [2, 3, 5, 10],\n",
    "                      \"features__lowercase\": [False, True],\n",
    "                      \"features__stop_words\": [None, stopwords],\n",
    "                      \"features__token_pattern\": [r'\\w+|\\,', None],\n",
    "                      \"clf__alpha\": [0.01, 0.1, 0.5, 1, 2]\n",
    "                    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report(results, n_top=5):\n",
    "    for i in range(1, n_top + 1):\n",
    "        candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
    "        for candidate in candidates:\n",
    "            print(\"Model with rank: {0}\".format(i))\n",
    "            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "                  results['mean_test_score'][candidate],\n",
    "                  results['std_test_score'][candidate]))\n",
    "            print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
    "            print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "catching classes that do not inherit from BaseException is not allowed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\", line 344, in __call__\n    return self.func(*args, **kwargs)\n  File \"/opt/conda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\", line 131, in __call__\n    return [func(*args, **kwargs) for func, args, kwargs in self.items]\n  File \"/opt/conda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\", line 131, in <listcomp>\n    return [func(*args, **kwargs) for func, args, kwargs in self.items]\n  File \"/opt/conda/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\", line 238, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/opt/conda/lib/python3.6/site-packages/sklearn/pipeline.py\", line 268, in fit\n    Xt, fit_params = self._fit(X, y, **fit_params)\n  File \"/opt/conda/lib/python3.6/site-packages/sklearn/pipeline.py\", line 234, in _fit\n    Xt = transform.fit_transform(Xt, y, **fit_params_steps[name])\n  File \"/opt/conda/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 1352, in fit_transform\n    X = super(TfidfVectorizer, self).fit_transform(raw_documents)\n  File \"/opt/conda/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 839, in fit_transform\n    self.fixed_vocabulary_)\n  File \"/opt/conda/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 755, in _count_vocab\n    analyze = self.build_analyzer()\n  File \"/opt/conda/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 238, in build_analyzer\n    tokenize = self.build_tokenizer()\n  File \"/opt/conda/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 215, in build_tokenizer\n    token_pattern = re.compile(self.token_pattern)\n  File \"/opt/conda/lib/python3.6/re.py\", line 233, in compile\n    return _compile(pattern, flags)\n  File \"/opt/conda/lib/python3.6/re.py\", line 300, in _compile\n    raise TypeError(\"first argument must be string or compiled pattern\")\nTypeError: first argument must be string or compiled pattern\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n    result = (True, func(*args, **kwds))\n  File \"/opt/conda/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\", line 353, in __call__\n    raise TransportableException(text, e_type)\nsklearn.externals.joblib.my_exceptions.TransportableException: TransportableException\n___________________________________________________________________________\nTypeError                                          Sun Nov 26 14:16:25 2017\nPID: 576                                Python 3.6.2: /opt/conda/bin/python\n...........................................................................\n/opt/conda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        self.items = [(<function _fit_and_score>, (Pipeline(steps=[('features', TfidfVectorizer(ana...lNB(alpha=1, class_prior=None, fit_prior=True))]), id\nid26305    This process, however, afforded me...my shoulder, and it ...\nName: text, dtype: object, id\nid26305    EAP\nid17569    HPL\nid11008    EAP\n...   EAP\nid00393    HPL\nName: author, dtype: object, make_scorer(log_loss, greater_is_better=False, needs_proba=True), array([ 6475,  6476,  6478, ..., 19576, 19577, 19578]), array([   0,    1,    2, ..., 6604, 6605, 6606]), 0, {'clf__alpha': 1, 'features__analyzer': 'word', 'features__lowercase': False, 'features__max_df': 0.7, 'features__min_df': 10, 'features__ngram_range': (1, 2), 'features__stop_words': None, 'features__token_pattern': None}), {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': True, 'return_times': True, 'return_train_score': True})]\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\n/opt/conda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in <listcomp>(.0=<list_iterator object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        func = <function _fit_and_score>\n        args = (Pipeline(steps=[('features', TfidfVectorizer(ana...lNB(alpha=1, class_prior=None, fit_prior=True))]), id\nid26305    This process, however, afforded me...my shoulder, and it ...\nName: text, dtype: object, id\nid26305    EAP\nid17569    HPL\nid11008    EAP\n...   EAP\nid00393    HPL\nName: author, dtype: object, make_scorer(log_loss, greater_is_better=False, needs_proba=True), array([ 6475,  6476,  6478, ..., 19576, 19577, 19578]), array([   0,    1,    2, ..., 6604, 6605, 6606]), 0, {'clf__alpha': 1, 'features__analyzer': 'word', 'features__lowercase': False, 'features__max_df': 0.7, 'features__min_df': 10, 'features__ngram_range': (1, 2), 'features__stop_words': None, 'features__token_pattern': None})\n        kwargs = {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': True, 'return_times': True, 'return_train_score': True}\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\n/opt/conda/lib/python3.6/site-packages/sklearn/model_selection/_validation.py in _fit_and_score(estimator=Pipeline(steps=[('features', TfidfVectorizer(ana...lNB(alpha=1, class_prior=None, fit_prior=True))]), X=id\nid26305    This process, however, afforded me...my shoulder, and it ...\nName: text, dtype: object, y=id\nid26305    EAP\nid17569    HPL\nid11008    EAP\n...   EAP\nid00393    HPL\nName: author, dtype: object, scorer=make_scorer(log_loss, greater_is_better=False, needs_proba=True), train=array([ 6475,  6476,  6478, ..., 19576, 19577, 19578]), test=array([   0,    1,    2, ..., 6604, 6605, 6606]), verbose=0, parameters={'clf__alpha': 1, 'features__analyzer': 'word', 'features__lowercase': False, 'features__max_df': 0.7, 'features__min_df': 10, 'features__ngram_range': (1, 2), 'features__stop_words': None, 'features__token_pattern': None}, fit_params={}, return_train_score=True, return_parameters=True, return_n_test_samples=True, return_times=True, error_score='raise')\n    233 \n    234     try:\n    235         if y_train is None:\n    236             estimator.fit(X_train, **fit_params)\n    237         else:\n--> 238             estimator.fit(X_train, y_train, **fit_params)\n        estimator.fit = <bound method Pipeline.fit of Pipeline(steps=[('...NB(alpha=1, class_prior=None, fit_prior=True))])>\n        X_train = id\nid15739    Its close resemblance to the medic...my shoulder, and it ...\nName: text, dtype: object\n        y_train = id\nid15739    EAP\nid19588    EAP\nid11059    EAP\n...   EAP\nid00393    HPL\nName: author, dtype: object\n        fit_params = {}\n    239 \n    240     except Exception as e:\n    241         # Note fit time as time until error\n    242         fit_time = time.time() - start_time\n\n...........................................................................\n/opt/conda/lib/python3.6/site-packages/sklearn/pipeline.py in fit(self=Pipeline(steps=[('features', TfidfVectorizer(ana...lNB(alpha=1, class_prior=None, fit_prior=True))]), X=id\nid15739    Its close resemblance to the medic...my shoulder, and it ...\nName: text, dtype: object, y=id\nid15739    EAP\nid19588    EAP\nid11059    EAP\n...   EAP\nid00393    HPL\nName: author, dtype: object, **fit_params={})\n    263         Returns\n    264         -------\n    265         self : Pipeline\n    266             This estimator\n    267         \"\"\"\n--> 268         Xt, fit_params = self._fit(X, y, **fit_params)\n        Xt = undefined\n        fit_params = {}\n        self._fit = <bound method Pipeline._fit of Pipeline(steps=[(...NB(alpha=1, class_prior=None, fit_prior=True))])>\n        X = id\nid15739    Its close resemblance to the medic...my shoulder, and it ...\nName: text, dtype: object\n        y = id\nid15739    EAP\nid19588    EAP\nid11059    EAP\n...   EAP\nid00393    HPL\nName: author, dtype: object\n    269         if self._final_estimator is not None:\n    270             self._final_estimator.fit(Xt, y, **fit_params)\n    271         return self\n    272 \n\n...........................................................................\n/opt/conda/lib/python3.6/site-packages/sklearn/pipeline.py in _fit(self=Pipeline(steps=[('features', TfidfVectorizer(ana...lNB(alpha=1, class_prior=None, fit_prior=True))]), X=id\nid15739    Its close resemblance to the medic...my shoulder, and it ...\nName: text, dtype: object, y=id\nid15739    EAP\nid19588    EAP\nid11059    EAP\n...   EAP\nid00393    HPL\nName: author, dtype: object, **fit_params={})\n    229         Xt = X\n    230         for name, transform in self.steps[:-1]:\n    231             if transform is None:\n    232                 pass\n    233             elif hasattr(transform, \"fit_transform\"):\n--> 234                 Xt = transform.fit_transform(Xt, y, **fit_params_steps[name])\n        Xt = id\nid15739    Its close resemblance to the medic...my shoulder, and it ...\nName: text, dtype: object\n        transform.fit_transform = <bound method TfidfVectorizer.fit_transform of T..., tokenizer=None, use_idf=True, vocabulary=None)>\n        y = id\nid15739    EAP\nid19588    EAP\nid11059    EAP\n...   EAP\nid00393    HPL\nName: author, dtype: object\n        fit_params_steps = {'clf': {}, 'features': {}}\n        name = 'features'\n    235             else:\n    236                 Xt = transform.fit(Xt, y, **fit_params_steps[name]) \\\n    237                               .transform(Xt)\n    238         if self._final_estimator is None:\n\n...........................................................................\n/opt/conda/lib/python3.6/site-packages/sklearn/feature_extraction/text.py in fit_transform(self=TfidfVectorizer(analyzer='word', binary=False, d...e, tokenizer=None, use_idf=True, vocabulary=None), raw_documents=id\nid15739    Its close resemblance to the medic...my shoulder, and it ...\nName: text, dtype: object, y=id\nid15739    EAP\nid19588    EAP\nid11059    EAP\n...   EAP\nid00393    HPL\nName: author, dtype: object)\n   1347         Returns\n   1348         -------\n   1349         X : sparse matrix, [n_samples, n_features]\n   1350             Tf-idf-weighted document-term matrix.\n   1351         \"\"\"\n-> 1352         X = super(TfidfVectorizer, self).fit_transform(raw_documents)\n        X = undefined\n        self.fit_transform = <bound method TfidfVectorizer.fit_transform of T..., tokenizer=None, use_idf=True, vocabulary=None)>\n        raw_documents = id\nid15739    Its close resemblance to the medic...my shoulder, and it ...\nName: text, dtype: object\n   1353         self._tfidf.fit(X)\n   1354         # X is already a transformed view of raw_documents so\n   1355         # we set copy to False\n   1356         return self._tfidf.transform(X, copy=False)\n\n...........................................................................\n/opt/conda/lib/python3.6/site-packages/sklearn/feature_extraction/text.py in fit_transform(self=TfidfVectorizer(analyzer='word', binary=False, d...e, tokenizer=None, use_idf=True, vocabulary=None), raw_documents=id\nid15739    Its close resemblance to the medic...my shoulder, and it ...\nName: text, dtype: object, y=None)\n    834         max_df = self.max_df\n    835         min_df = self.min_df\n    836         max_features = self.max_features\n    837 \n    838         vocabulary, X = self._count_vocab(raw_documents,\n--> 839                                           self.fixed_vocabulary_)\n        self.fixed_vocabulary_ = False\n    840 \n    841         if self.binary:\n    842             X.data.fill(1)\n    843 \n\n...........................................................................\n/opt/conda/lib/python3.6/site-packages/sklearn/feature_extraction/text.py in _count_vocab(self=TfidfVectorizer(analyzer='word', binary=False, d...e, tokenizer=None, use_idf=True, vocabulary=None), raw_documents=id\nid15739    Its close resemblance to the medic...my shoulder, and it ...\nName: text, dtype: object, fixed_vocab=False)\n    750         else:\n    751             # Add a new value when a new vocabulary item is seen\n    752             vocabulary = defaultdict()\n    753             vocabulary.default_factory = vocabulary.__len__\n    754 \n--> 755         analyze = self.build_analyzer()\n        analyze = undefined\n        self.build_analyzer = <bound method VectorizerMixin.build_analyzer of ..., tokenizer=None, use_idf=True, vocabulary=None)>\n    756         j_indices = []\n    757         indptr = _make_int_array()\n    758         values = _make_int_array()\n    759         indptr.append(0)\n\n...........................................................................\n/opt/conda/lib/python3.6/site-packages/sklearn/feature_extraction/text.py in build_analyzer(self=TfidfVectorizer(analyzer='word', binary=False, d...e, tokenizer=None, use_idf=True, vocabulary=None))\n    233             return lambda doc: self._char_wb_ngrams(\n    234                 preprocess(self.decode(doc)))\n    235 \n    236         elif self.analyzer == 'word':\n    237             stop_words = self.get_stop_words()\n--> 238             tokenize = self.build_tokenizer()\n        self.build_tokenizer = <bound method VectorizerMixin.build_tokenizer of..., tokenizer=None, use_idf=True, vocabulary=None)>\n    239 \n    240             return lambda doc: self._word_ngrams(\n    241                 tokenize(preprocess(self.decode(doc))), stop_words)\n    242 \n\n...........................................................................\n/opt/conda/lib/python3.6/site-packages/sklearn/feature_extraction/text.py in build_tokenizer(self=TfidfVectorizer(analyzer='word', binary=False, d...e, tokenizer=None, use_idf=True, vocabulary=None))\n    210 \n    211     def build_tokenizer(self):\n    212         \"\"\"Return a function that splits a string into a sequence of tokens\"\"\"\n    213         if self.tokenizer is not None:\n    214             return self.tokenizer\n--> 215         token_pattern = re.compile(self.token_pattern)\n        self.token_pattern = None\n    216         return lambda doc: token_pattern.findall(doc)\n    217 \n    218     def get_stop_words(self):\n    219         \"\"\"Build or fetch the effective stop words list\"\"\"\n\n...........................................................................\n/opt/conda/lib/python3.6/re.py in compile(pattern=None, flags=0)\n    228     Empty matches are included in the result.\"\"\"\n    229     return _compile(pattern, flags).finditer(string)\n    230 \n    231 def compile(pattern, flags=0):\n    232     \"Compile a regular expression pattern, returning a pattern object.\"\n--> 233     return _compile(pattern, flags)\n        pattern = None\n        flags = 0\n    234 \n    235 def purge():\n    236     \"Clear the regular expression caches\"\n    237     _cache.clear()\n\n...........................................................................\n/opt/conda/lib/python3.6/re.py in _compile(pattern=None, flags=0)\n    295         if flags:\n    296             raise ValueError(\n    297                 \"cannot process flags argument with a compiled pattern\")\n    298         return pattern\n    299     if not sre_compile.isstring(pattern):\n--> 300         raise TypeError(\"first argument must be string or compiled pattern\")\n    301     p = sre_compile.compile(pattern, flags)\n    302     if not (flags & DEBUG):\n    303         if len(_cache) >= _MAXCACHE:\n    304             _cache.clear()\n\nTypeError: first argument must be string or compiled pattern\n___________________________________________________________________________\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTransportableException\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    681\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m'timeout'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgetfullargspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 682\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    683\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTransportableException\u001b[0m: TransportableException\n___________________________________________________________________________\nTypeError                                          Sun Nov 26 14:16:25 2017\nPID: 576                                Python 3.6.2: /opt/conda/bin/python\n...........................................................................\n/opt/conda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        self.items = [(<function _fit_and_score>, (Pipeline(steps=[('features', TfidfVectorizer(ana...lNB(alpha=1, class_prior=None, fit_prior=True))]), id\nid26305    This process, however, afforded me...my shoulder, and it ...\nName: text, dtype: object, id\nid26305    EAP\nid17569    HPL\nid11008    EAP\n...   EAP\nid00393    HPL\nName: author, dtype: object, make_scorer(log_loss, greater_is_better=False, needs_proba=True), array([ 6475,  6476,  6478, ..., 19576, 19577, 19578]), array([   0,    1,    2, ..., 6604, 6605, 6606]), 0, {'clf__alpha': 1, 'features__analyzer': 'word', 'features__lowercase': False, 'features__max_df': 0.7, 'features__min_df': 10, 'features__ngram_range': (1, 2), 'features__stop_words': None, 'features__token_pattern': None}), {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': True, 'return_times': True, 'return_train_score': True})]\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\n/opt/conda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in <listcomp>(.0=<list_iterator object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        func = <function _fit_and_score>\n        args = (Pipeline(steps=[('features', TfidfVectorizer(ana...lNB(alpha=1, class_prior=None, fit_prior=True))]), id\nid26305    This process, however, afforded me...my shoulder, and it ...\nName: text, dtype: object, id\nid26305    EAP\nid17569    HPL\nid11008    EAP\n...   EAP\nid00393    HPL\nName: author, dtype: object, make_scorer(log_loss, greater_is_better=False, needs_proba=True), array([ 6475,  6476,  6478, ..., 19576, 19577, 19578]), array([   0,    1,    2, ..., 6604, 6605, 6606]), 0, {'clf__alpha': 1, 'features__analyzer': 'word', 'features__lowercase': False, 'features__max_df': 0.7, 'features__min_df': 10, 'features__ngram_range': (1, 2), 'features__stop_words': None, 'features__token_pattern': None})\n        kwargs = {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': True, 'return_times': True, 'return_train_score': True}\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\n/opt/conda/lib/python3.6/site-packages/sklearn/model_selection/_validation.py in _fit_and_score(estimator=Pipeline(steps=[('features', TfidfVectorizer(ana...lNB(alpha=1, class_prior=None, fit_prior=True))]), X=id\nid26305    This process, however, afforded me...my shoulder, and it ...\nName: text, dtype: object, y=id\nid26305    EAP\nid17569    HPL\nid11008    EAP\n...   EAP\nid00393    HPL\nName: author, dtype: object, scorer=make_scorer(log_loss, greater_is_better=False, needs_proba=True), train=array([ 6475,  6476,  6478, ..., 19576, 19577, 19578]), test=array([   0,    1,    2, ..., 6604, 6605, 6606]), verbose=0, parameters={'clf__alpha': 1, 'features__analyzer': 'word', 'features__lowercase': False, 'features__max_df': 0.7, 'features__min_df': 10, 'features__ngram_range': (1, 2), 'features__stop_words': None, 'features__token_pattern': None}, fit_params={}, return_train_score=True, return_parameters=True, return_n_test_samples=True, return_times=True, error_score='raise')\n    233 \n    234     try:\n    235         if y_train is None:\n    236             estimator.fit(X_train, **fit_params)\n    237         else:\n--> 238             estimator.fit(X_train, y_train, **fit_params)\n        estimator.fit = <bound method Pipeline.fit of Pipeline(steps=[('...NB(alpha=1, class_prior=None, fit_prior=True))])>\n        X_train = id\nid15739    Its close resemblance to the medic...my shoulder, and it ...\nName: text, dtype: object\n        y_train = id\nid15739    EAP\nid19588    EAP\nid11059    EAP\n...   EAP\nid00393    HPL\nName: author, dtype: object\n        fit_params = {}\n    239 \n    240     except Exception as e:\n    241         # Note fit time as time until error\n    242         fit_time = time.time() - start_time\n\n...........................................................................\n/opt/conda/lib/python3.6/site-packages/sklearn/pipeline.py in fit(self=Pipeline(steps=[('features', TfidfVectorizer(ana...lNB(alpha=1, class_prior=None, fit_prior=True))]), X=id\nid15739    Its close resemblance to the medic...my shoulder, and it ...\nName: text, dtype: object, y=id\nid15739    EAP\nid19588    EAP\nid11059    EAP\n...   EAP\nid00393    HPL\nName: author, dtype: object, **fit_params={})\n    263         Returns\n    264         -------\n    265         self : Pipeline\n    266             This estimator\n    267         \"\"\"\n--> 268         Xt, fit_params = self._fit(X, y, **fit_params)\n        Xt = undefined\n        fit_params = {}\n        self._fit = <bound method Pipeline._fit of Pipeline(steps=[(...NB(alpha=1, class_prior=None, fit_prior=True))])>\n        X = id\nid15739    Its close resemblance to the medic...my shoulder, and it ...\nName: text, dtype: object\n        y = id\nid15739    EAP\nid19588    EAP\nid11059    EAP\n...   EAP\nid00393    HPL\nName: author, dtype: object\n    269         if self._final_estimator is not None:\n    270             self._final_estimator.fit(Xt, y, **fit_params)\n    271         return self\n    272 \n\n...........................................................................\n/opt/conda/lib/python3.6/site-packages/sklearn/pipeline.py in _fit(self=Pipeline(steps=[('features', TfidfVectorizer(ana...lNB(alpha=1, class_prior=None, fit_prior=True))]), X=id\nid15739    Its close resemblance to the medic...my shoulder, and it ...\nName: text, dtype: object, y=id\nid15739    EAP\nid19588    EAP\nid11059    EAP\n...   EAP\nid00393    HPL\nName: author, dtype: object, **fit_params={})\n    229         Xt = X\n    230         for name, transform in self.steps[:-1]:\n    231             if transform is None:\n    232                 pass\n    233             elif hasattr(transform, \"fit_transform\"):\n--> 234                 Xt = transform.fit_transform(Xt, y, **fit_params_steps[name])\n        Xt = id\nid15739    Its close resemblance to the medic...my shoulder, and it ...\nName: text, dtype: object\n        transform.fit_transform = <bound method TfidfVectorizer.fit_transform of T..., tokenizer=None, use_idf=True, vocabulary=None)>\n        y = id\nid15739    EAP\nid19588    EAP\nid11059    EAP\n...   EAP\nid00393    HPL\nName: author, dtype: object\n        fit_params_steps = {'clf': {}, 'features': {}}\n        name = 'features'\n    235             else:\n    236                 Xt = transform.fit(Xt, y, **fit_params_steps[name]) \\\n    237                               .transform(Xt)\n    238         if self._final_estimator is None:\n\n...........................................................................\n/opt/conda/lib/python3.6/site-packages/sklearn/feature_extraction/text.py in fit_transform(self=TfidfVectorizer(analyzer='word', binary=False, d...e, tokenizer=None, use_idf=True, vocabulary=None), raw_documents=id\nid15739    Its close resemblance to the medic...my shoulder, and it ...\nName: text, dtype: object, y=id\nid15739    EAP\nid19588    EAP\nid11059    EAP\n...   EAP\nid00393    HPL\nName: author, dtype: object)\n   1347         Returns\n   1348         -------\n   1349         X : sparse matrix, [n_samples, n_features]\n   1350             Tf-idf-weighted document-term matrix.\n   1351         \"\"\"\n-> 1352         X = super(TfidfVectorizer, self).fit_transform(raw_documents)\n        X = undefined\n        self.fit_transform = <bound method TfidfVectorizer.fit_transform of T..., tokenizer=None, use_idf=True, vocabulary=None)>\n        raw_documents = id\nid15739    Its close resemblance to the medic...my shoulder, and it ...\nName: text, dtype: object\n   1353         self._tfidf.fit(X)\n   1354         # X is already a transformed view of raw_documents so\n   1355         # we set copy to False\n   1356         return self._tfidf.transform(X, copy=False)\n\n...........................................................................\n/opt/conda/lib/python3.6/site-packages/sklearn/feature_extraction/text.py in fit_transform(self=TfidfVectorizer(analyzer='word', binary=False, d...e, tokenizer=None, use_idf=True, vocabulary=None), raw_documents=id\nid15739    Its close resemblance to the medic...my shoulder, and it ...\nName: text, dtype: object, y=None)\n    834         max_df = self.max_df\n    835         min_df = self.min_df\n    836         max_features = self.max_features\n    837 \n    838         vocabulary, X = self._count_vocab(raw_documents,\n--> 839                                           self.fixed_vocabulary_)\n        self.fixed_vocabulary_ = False\n    840 \n    841         if self.binary:\n    842             X.data.fill(1)\n    843 \n\n...........................................................................\n/opt/conda/lib/python3.6/site-packages/sklearn/feature_extraction/text.py in _count_vocab(self=TfidfVectorizer(analyzer='word', binary=False, d...e, tokenizer=None, use_idf=True, vocabulary=None), raw_documents=id\nid15739    Its close resemblance to the medic...my shoulder, and it ...\nName: text, dtype: object, fixed_vocab=False)\n    750         else:\n    751             # Add a new value when a new vocabulary item is seen\n    752             vocabulary = defaultdict()\n    753             vocabulary.default_factory = vocabulary.__len__\n    754 \n--> 755         analyze = self.build_analyzer()\n        analyze = undefined\n        self.build_analyzer = <bound method VectorizerMixin.build_analyzer of ..., tokenizer=None, use_idf=True, vocabulary=None)>\n    756         j_indices = []\n    757         indptr = _make_int_array()\n    758         values = _make_int_array()\n    759         indptr.append(0)\n\n...........................................................................\n/opt/conda/lib/python3.6/site-packages/sklearn/feature_extraction/text.py in build_analyzer(self=TfidfVectorizer(analyzer='word', binary=False, d...e, tokenizer=None, use_idf=True, vocabulary=None))\n    233             return lambda doc: self._char_wb_ngrams(\n    234                 preprocess(self.decode(doc)))\n    235 \n    236         elif self.analyzer == 'word':\n    237             stop_words = self.get_stop_words()\n--> 238             tokenize = self.build_tokenizer()\n        self.build_tokenizer = <bound method VectorizerMixin.build_tokenizer of..., tokenizer=None, use_idf=True, vocabulary=None)>\n    239 \n    240             return lambda doc: self._word_ngrams(\n    241                 tokenize(preprocess(self.decode(doc))), stop_words)\n    242 \n\n...........................................................................\n/opt/conda/lib/python3.6/site-packages/sklearn/feature_extraction/text.py in build_tokenizer(self=TfidfVectorizer(analyzer='word', binary=False, d...e, tokenizer=None, use_idf=True, vocabulary=None))\n    210 \n    211     def build_tokenizer(self):\n    212         \"\"\"Return a function that splits a string into a sequence of tokens\"\"\"\n    213         if self.tokenizer is not None:\n    214             return self.tokenizer\n--> 215         token_pattern = re.compile(self.token_pattern)\n        self.token_pattern = None\n    216         return lambda doc: token_pattern.findall(doc)\n    217 \n    218     def get_stop_words(self):\n    219         \"\"\"Build or fetch the effective stop words list\"\"\"\n\n...........................................................................\n/opt/conda/lib/python3.6/re.py in compile(pattern=None, flags=0)\n    228     Empty matches are included in the result.\"\"\"\n    229     return _compile(pattern, flags).finditer(string)\n    230 \n    231 def compile(pattern, flags=0):\n    232     \"Compile a regular expression pattern, returning a pattern object.\"\n--> 233     return _compile(pattern, flags)\n        pattern = None\n        flags = 0\n    234 \n    235 def purge():\n    236     \"Clear the regular expression caches\"\n    237     _cache.clear()\n\n...........................................................................\n/opt/conda/lib/python3.6/re.py in _compile(pattern=None, flags=0)\n    295         if flags:\n    296             raise ValueError(\n    297                 \"cannot process flags argument with a compiled pattern\")\n    298         return pattern\n    299     if not sre_compile.isstring(pattern):\n--> 300         raise TypeError(\"first argument must be string or compiled pattern\")\n    301     p = sre_compile.compile(pattern, flags)\n    302     if not (flags & DEBUG):\n    303         if len(_cache) >= _MAXCACHE:\n    304             _cache.clear()\n\nTypeError: first argument must be string or compiled pattern\n___________________________________________________________________________",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/externals/joblib/pool.py\u001b[0m in \u001b[0;36mterminate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    605\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 606\u001b[0;31m                 \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMemmapingPool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mterminate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    607\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mterminate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_worker_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTERMINATE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_terminate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/multiprocessing/util.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, wr, _finalizer_registry, sub_debug, getpid)\u001b[0m\n\u001b[1;32m    185\u001b[0m                           self._callback, self._args, self._kwargs)\n\u001b[0;32m--> 186\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_weakref\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_args\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36m_terminate_pool\u001b[0;34m(cls, taskqueue, inqueue, outqueue, pool, worker_handler, task_handler, result_handler, cache)\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mthreading\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtask_handler\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m             \u001b[0mtask_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    594\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1055\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1056\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1057\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1071\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_stopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1072\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1073\u001b[0m             \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-3066d2b7d0fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m                                        n_iter=20, cv=3, n_jobs=4)\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mrandom_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauthor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0mreport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv_results_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m   1188\u001b[0m                                           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1189\u001b[0m                                           random_state=self.random_state)\n\u001b[0;32m-> 1190\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampled_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, groups, parameter_iterable)\u001b[0m\n\u001b[1;32m    562\u001b[0m                                   \u001b[0mreturn_times\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_parameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m                                   error_score=self.error_score)\n\u001b[0;32m--> 564\u001b[0;31m           \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameter_iterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    565\u001b[0m           for train, test in cv_iter)\n\u001b[1;32m    566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    766\u001b[0m                 \u001b[0;31m# consumption.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 768\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    769\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    716\u001b[0m                     \u001b[0;31m# scheduling.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m                     \u001b[0mensure_ready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_managed_backend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 718\u001b[0;31m                     \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabort_everything\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_ready\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_ready\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    719\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mabort_everything\u001b[0;34m(self, ensure_ready)\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mabort_everything\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_ready\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0;34m\"\"\"Shutdown the pool and restart a new one with the same parameters\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mterminate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mensure_ready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m             self.configure(n_jobs=self.parallel.n_jobs, parallel=self.parallel,\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mterminate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    315\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mterminate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0;34m\"\"\"Shutdown the process or thread pool\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 317\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMultiprocessingBackend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mterminate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    318\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mJOBLIB_SPAWNED_PROCESS\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mJOBLIB_SPAWNED_PROCESS\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mterminate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mterminate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# terminate does a join()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/externals/joblib/pool.py\u001b[0m in \u001b[0;36mterminate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    606\u001b[0m                 \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMemmapingPool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mterminate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 608\u001b[0;31m             \u001b[0;32mexcept\u001b[0m \u001b[0mWindowsError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    609\u001b[0m                 \u001b[0;31m# Workaround  occasional \"[Error 5] Access is denied\" issue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m                 \u001b[0;31m# when trying to terminate a process under windows.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: catching classes that do not inherit from BaseException is not allowed"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# from nltk.corpus import stopwords\n",
    "# stopset = set(stopwords.words('english'))\n",
    "\n",
    "# from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('features', TfidfVectorizer(\n",
    "#         ngram_range=(1,2), min_df=2, \n",
    "#                                 max_df=0.8, lowercase=False, \n",
    "#                                 token_pattern=r'\\w+|\\,', stop_words=stopset\n",
    "                                )),\n",
    "    ('clf', MultinomialNB(\n",
    "#         alpha=0.01\n",
    "    )),\n",
    "])\n",
    "\n",
    "# print(pipeline.named_steps['features'])\n",
    "\n",
    "# print(cross_val_score(pipeline, train.text, train.author, cv=3, n_jobs=3))\n",
    "# print(cross_val_score(pipeline, train.text, train.author, cv=3, n_jobs=3, \n",
    "#                       scoring='neg_log_loss'))\n",
    "\n",
    "random_search = RandomizedSearchCV(pipeline, param_distributions=params, \n",
    "                                       scoring='neg_log_loss',\n",
    "                                       n_iter=20, cv=3, n_jobs=4)\n",
    "\n",
    "random_search.fit(train.text, train.author)\n",
    "report(random_search.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = pipeline.fit(train.text, train.author)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1.98283890e-02   3.81339586e-03   9.76358215e-01]\n",
      " [  9.96057571e-01   3.11816161e-03   8.24267151e-04]\n",
      " [  7.44699788e-03   9.90084289e-01   2.46871320e-03]\n",
      " [  8.24766674e-01   1.73950931e-01   1.28239521e-03]\n",
      " [  5.33013756e-01   3.65558628e-01   1.01427616e-01]\n",
      " [  9.14797902e-01   8.29368343e-02   2.26526379e-03]\n",
      " [  8.60877068e-01   1.35091168e-01   4.03176395e-03]\n",
      " [  1.20976104e-02   1.99588590e-01   7.88313799e-01]\n",
      " [  9.92430401e-01   7.55044856e-03   1.91502121e-05]\n",
      " [  7.66137344e-01   9.48152515e-02   1.39047405e-01]]\n"
     ]
    }
   ],
   "source": [
    "print(pipeline.predict_proba(test[:10].text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = pipeline.predict_proba(test.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['EAP' 'HPL' 'MWS']\n"
     ]
    }
   ],
   "source": [
    "print(pipeline.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EAP</th>\n",
       "      <th>MWS</th>\n",
       "      <th>HPL</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>id02310</th>\n",
       "      <td>0.019828</td>\n",
       "      <td>0.003813</td>\n",
       "      <td>0.976358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id24541</th>\n",
       "      <td>0.996058</td>\n",
       "      <td>0.003118</td>\n",
       "      <td>0.000824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id00134</th>\n",
       "      <td>0.007447</td>\n",
       "      <td>0.990084</td>\n",
       "      <td>0.002469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id27757</th>\n",
       "      <td>0.824767</td>\n",
       "      <td>0.173951</td>\n",
       "      <td>0.001282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id04081</th>\n",
       "      <td>0.533014</td>\n",
       "      <td>0.365559</td>\n",
       "      <td>0.101428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id27337</th>\n",
       "      <td>0.914798</td>\n",
       "      <td>0.082937</td>\n",
       "      <td>0.002265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id24265</th>\n",
       "      <td>0.860877</td>\n",
       "      <td>0.135091</td>\n",
       "      <td>0.004032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id25917</th>\n",
       "      <td>0.012098</td>\n",
       "      <td>0.199589</td>\n",
       "      <td>0.788314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id04951</th>\n",
       "      <td>0.992430</td>\n",
       "      <td>0.007550</td>\n",
       "      <td>0.000019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id14549</th>\n",
       "      <td>0.766137</td>\n",
       "      <td>0.094815</td>\n",
       "      <td>0.139047</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              EAP       MWS       HPL\n",
       "id                                   \n",
       "id02310  0.019828  0.003813  0.976358\n",
       "id24541  0.996058  0.003118  0.000824\n",
       "id00134  0.007447  0.990084  0.002469\n",
       "id27757  0.824767  0.173951  0.001282\n",
       "id04081  0.533014  0.365559  0.101428\n",
       "id27337  0.914798  0.082937  0.002265\n",
       "id24265  0.860877  0.135091  0.004032\n",
       "id25917  0.012098  0.199589  0.788314\n",
       "id04951  0.992430  0.007550  0.000019\n",
       "id14549  0.766137  0.094815  0.139047"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submit_file = pd.DataFrame(test_predictions, columns=['EAP', 'MWS', 'HPL'], index=test.index)\n",
    "submit_file.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_file.to_csv(\"data/spooky-authors/predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
